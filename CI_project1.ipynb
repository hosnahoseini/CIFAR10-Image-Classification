{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9391c1af-b8af-4e26-b23e-21f71d613597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcde7ab2-348b-4c70-831e-0a212fe12dfd",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c91a8661-a10b-4933-b8b8-4d2ce33f7c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(images_file, label_file):\n",
    "\n",
    "    train_images_file = open(images_file, 'rb')\n",
    "    train_images_file.seek(4)\n",
    "    num_of_train_images = int.from_bytes(train_images_file.read(4), 'big')\n",
    "    train_images_file.seek(16)\n",
    "\n",
    "    train_labels_file = open(label_file, 'rb')\n",
    "    train_labels_file.seek(8)\n",
    "\n",
    "    data_set = []\n",
    "    for n in range(num_of_train_images):\n",
    "        image = np.zeros((784, 1))\n",
    "        for i in range(784):\n",
    "            image[i, 0] = int.from_bytes(train_images_file.read(1), 'big') / 256\n",
    "\n",
    "        label_value = int.from_bytes(train_labels_file.read(1), 'big')\n",
    "        label = np.zeros((10, 1))\n",
    "        label[label_value, 0] = 1\n",
    "\n",
    "        data_set.append((image, label))\n",
    "\n",
    "    return data_set\n",
    "\n",
    "train_set = read_data('train-images.idx3-ubyte', 'train-labels.idx1-ubyte')\n",
    "test_set = read_data('t10k-images.idx3-ubyte', 't10k-labels.idx1-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd29a2c-5b4d-48e1-8c2e-0fee273ef00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting an image\n",
    "\n",
    "def show_image(img):\n",
    "    image = img.reshape((28, 28))\n",
    "    plt.imshow(image, 'gray')\n",
    "    \n",
    "show_image(train_set[0][0])\n",
    "print(f\"label: {np.argmax(train_set[0][1])}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289a5886-2d6e-46db-a43f-15b0a9ac1e27",
   "metadata": {},
   "source": [
    "## Feed forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74d26c6a-a9ba-4224-a671-16341de517a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dim = (784, 16, 16, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "677dbbad-025d-4247-a05f-8187b9f87940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_W_b(layers_dim):\n",
    "    n0, n1, n2, n3 = layers_dim\n",
    "    # Initialize W from standard normal distribution\n",
    "    W1 = np.random.randn(n1, n0)\n",
    "    W2 = np.random.randn(n2, n1)\n",
    "    W3 = np.random.randn(n3, n2)\n",
    "    \n",
    "    b1 = np.zeros((n1, 1))\n",
    "    b2 = np.zeros((n2, 1))\n",
    "    b3 = np.zeros((n3, 1))\n",
    "    param = W1, W2, W3, b1, b2, b3 \n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54ab681b-49cc-4c5d-93be-41ce4a51708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77a0ade0-b3d3-48e0-9ae8-73bc10eb190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b):\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    A = sigmoid(Z)\n",
    "    return Z, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9b954e3-c833-4252-ab4f-7b3abb704bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(A0, param):\n",
    "    W1, W2, W3, b1, b2, b3 = param\n",
    "    Z1, A1 = linear_activation_forward(A0, W1, b1)\n",
    "    Z2, A2 = linear_activation_forward(A1, W2, b2)\n",
    "    Z3, A3 = linear_activation_forward(A2, W3, b3)\n",
    "    return A3, Z3, A2, Z2, A1, Z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1669851f-4977-430c-8036-a53e8615e8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(train_data, n, param):\n",
    "    count = 0\n",
    "    n = 100\n",
    "\n",
    "    for train_data in train_set[:n]:\n",
    "        A0 = train_data[0]\n",
    "        A3, *_ = feed_forward(A0, param)\n",
    "\n",
    "        predicted_number = np.argmax(A3)\n",
    "        real_number = np.argmax(train_data[1])\n",
    "\n",
    "        if predicted_number == real_number:\n",
    "            count += 1\n",
    "\n",
    "    return {count / n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e1d9e13-901b-4ea7-92e7-0b00de291d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: {0.14}\n"
     ]
    }
   ],
   "source": [
    "param = init_W_b(layers_dim)\n",
    "print(f\"Accuracy: {calculate_accuracy(train_set, 100, param)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cc7c5f-cea5-4f0b-9b7c-6474b9d398dc",
   "metadata": {},
   "source": [
    "## Back propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf788a9a-7563-4173-861e-5bb35b3d80cf",
   "metadata": {},
   "source": [
    "\n",
    "### 3rd layer\n",
    "##### Weight\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial w_{jk}^{(3)}} = \\displaystyle \\frac{\\partial Cost}{\\partial a_{j}^{(3)}} \\times \\displaystyle \\frac{\\partial a_{j}^{(3)}}{\\partial z_{j}^{(3)}} \\times \\displaystyle \\frac{\\partial z_{j}^{(3)}}{\\partial w_{jk}^{(3)}} $$\n",
    "\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial w_{jk}^{(3)}} = 2(a_{j}^{(3)} - y_{j}) \\times \\sigma^{'}(z_{j}^{(3)})\\times a_{k}^{(2)}$$\n",
    "\n",
    "##### Bias\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial b_{j}^{(3)}} = \\displaystyle \\frac{\\partial Cost}{\\partial a_{j}^{(3)}} \\times \\displaystyle \\frac{\\partial a_{j}^{(3)}}{\\partial z_{j}^{(3)}} \\times \\displaystyle \\frac{\\partial z_{j}^{(3)}}{\\partial b_{j}^{(3)}} $$\n",
    "\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial b_{j}^{(3)}} = 2(a_{j}^{(3)} - y_{j}) \\times \\sigma^{'}(z_{j}^{(3)})\\times 1$$\n",
    "\n",
    "##### Activation\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial a_{k}^{(2)}} = \\sum_{j=0}^{9} \\displaystyle \\frac{\\partial Cost}{\\partial a_{j}^{(3)}} \\times \\displaystyle \\frac{\\partial a_{j}^{(3)}}{\\partial z_{j}^{(3)}} \\times \\displaystyle \\frac{\\partial z_{j}^{(3)}}{\\partial a_{k}^{(2)}} $$\n",
    "\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial a_{k}^{(2)}} = \\sum_{j=0}^{9} (2(a_{j}^{(3)} - y_{j}) \\times \\sigma^{'}(z_{j}^{(3)})\\times w_{jk}^{(3)}) $$\n",
    "\n",
    "### 2nd layer\n",
    "##### Weight\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial w_{km}^{(2)}} = \\displaystyle \\frac{\\partial Cost}{\\partial a_{k}^{(2)}} \\times \\displaystyle \\frac{\\partial a_{k}^{(2)}}{\\partial z_{k}^{(2)}} \\times \\displaystyle \\frac{\\partial z_{k}^{(2)}}{\\partial w_{km}^{(2)}} $$\n",
    "\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial w_{km}^{(2)}} =  \\displaystyle \\frac{\\partial Cost}{\\partial a_{k}^{(2)}} \\times \\sigma^{'}(z_{k}^{(2)})\\times a_{m}^{(1)}$$\n",
    "\n",
    "##### Bias\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial b_{k}^{(2)}} = \\displaystyle \\frac{\\partial Cost}{\\partial a_{k}^{(2)}} \\times \\displaystyle \\frac{\\partial a_{k}^{(2)}}{\\partial z_{k}^{(2)}} \\times \\displaystyle \\frac{\\partial z_{k}^{(2)}}{\\partial b_{k}^{(2)}} $$\n",
    "\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial b_{k}^{(2)}} = \\displaystyle \\frac{\\partial Cost}{\\partial a_{k}^{(2)}} \\times \\sigma^{'}(z_{k}^{(2)})\\times 1$$\n",
    "\n",
    "##### Activation\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial a_{m}^{(1)}} = \\sum_{k=0}^{15} \\displaystyle \\frac{\\partial Cost}{\\partial a_{k}^{(2)}} \\times \\displaystyle \\frac{\\partial a_{k}^{(2)}}{\\partial z_{k}^{(2)}} \\times \\displaystyle \\frac{\\partial z_{k}^{(2)}}{\\partial a_{m}^{(1)}} $$\n",
    "\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial a_{m}^{(1)}} = \\sum_{k=0}^{15} (\\displaystyle \\frac{\\partial Cost}{\\partial a_{k}^{(2)}} \\times \\sigma^{'}(z_{k}^{(2)})\\times w_{km}^{(2)}) $$\n",
    "\n",
    "### first layer\n",
    "##### Weight\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial w_{mv}^{(1)}} = \\displaystyle \\frac{\\partial Cost}{\\partial a_{m}^{(1)}} \\times \\displaystyle \\frac{\\partial a_{m}^{(1)}}{\\partial z_{m}^{(1)}} \\times \\displaystyle \\frac{\\partial z_{m}^{(1)}}{\\partial w_{mv}^{(1)}} $$\n",
    "\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial w_{mv}^{(1)}} =  \\displaystyle \\frac{\\partial Cost}{\\partial a_{m}^{(1)}} \\times \\sigma^{'}(z_{m}^{(1)})\\times a_{v}^{(0)}$$\n",
    "\n",
    "##### Bias\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial b_{m}^{(1)}} = \\displaystyle \\frac{\\partial Cost}{\\partial a_{m}^{(1)}} \\times \\displaystyle \\frac{\\partial a_{m}^{(1)}}{\\partial z_{m}^{(1)}} \\times \\displaystyle \\frac{\\partial z_{m}^{(1)}}{\\partial b_{m}^{(1)}} $$\n",
    "\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial b_{m}^{(1)}} = \\displaystyle \\frac{\\partial Cost}{\\partial a_{m}^{(1)}} \\times \\sigma^{'}(z_{m}^{(1)})\\times 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0f366aa-ed34-4dc9-af09-7696deaf69a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivative of sigmoid\n",
    "def derivative_activation(x):\n",
    "    return (sigmoid(x) * (1 - sigmoid(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "649b0b43-8ebd-4980-a07c-b3e160c3295c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_grad_W_b_in_layer(layers_dim, activations, param, image, grads):\n",
    "    n0, n1, n2, n3 = layers_dim\n",
    "    A3, Z3, A2, Z2, A1, Z1 = activations\n",
    "    W1, W2, W3, b1, b2, b3 = param\n",
    "    grad_W1, grad_W2, grad_W3, grad_b1, grad_b2, grad_b3 = grads\n",
    "    \n",
    "    # layer 3\n",
    "    for j in range(n3):\n",
    "        for k in range(n2):\n",
    "            grad_W3[j][k] += (2 * (A3[j][0] - image[1][j])) * derivative_activation(Z3[j][0]) * (A2[k][0])\n",
    "            grad_b3[j][0] += (2 * (A3[j][0] - image[1][j])) * derivative_activation(Z3[j][0])\n",
    "\n",
    "    grad_A2 = np.zeros((n2, 1))\n",
    "    for k in range(n2):\n",
    "        for j in range(n3):\n",
    "            grad_A2[k][0] += (2 * (A3[j][0] - image[1][j])) * derivative_activation(Z3[j][0]) * W3[j][k]\n",
    "\n",
    "    # layer 2\n",
    "    for j in range(n2):\n",
    "        for k in range(n1):\n",
    "            grad_W2[j][k] += (grad_A2[j][0]) * derivative_activation(Z2[j][0]) * A1[k][0]\n",
    "            grad_b2[j][0] += (grad_A2[j][0]) * derivative_activation(Z2[j][0]) \n",
    "\n",
    "    grad_A1 = np.zeros((n1, 1))\n",
    "    for k in range(n1):\n",
    "        for j in range(n2):\n",
    "            grad_A1[k][0] += W2[j][k] * derivative_activation(Z2[j][0]) * grad_A2[j][0]\n",
    "\n",
    "    # layer 1\n",
    "    for j in range(n1):\n",
    "        for k in range(n0):\n",
    "            grad_W1[j][k] += grad_A1[j][0] * derivative_activation(Z1[j][0])  * image[0][k]\n",
    "            grad_b1[j][0] += grad_A1[j][0] * derivative_activation(Z1[j][0])\n",
    "\n",
    "    return grad_W3, grad_W2, grad_W1, grad_b3, grad_b2, grad_b1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f537408b-70c4-4338-863d-f42174f40c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update W and b (end of batch)\n",
    "def update_W_b(learning_rate, grad_W, W, grad_b, b, batch_size):\n",
    "    W -= (learning_rate * (grad_W / batch_size))\n",
    "    b -= (learning_rate * (grad_b / batch_size))\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "544063af-2240-492b-8200-6b946b6615b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(predicted, real):\n",
    "    cost = 0\n",
    "    for j in range(10):\n",
    "        cost += np.power((predicted[j] - real[j]), 2)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bd82ba5-8017-4200-a771-a7dd1a86544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_grad_W_b(layers_dim):\n",
    "    n0, n1, n2, n3 = layers_dim\n",
    "    \n",
    "    grad_W1 = np.zeros((n1, n0))\n",
    "    grad_W2 = np.zeros((n2, n1))\n",
    "    grad_W3 = np.zeros((n3, n2))\n",
    "\n",
    "    grad_b1 = np.zeros((n1, 1))\n",
    "    grad_b2 = np.zeros((n2, 1))\n",
    "    grad_b3 = np.zeros((n3, 1))\n",
    "    \n",
    "    grads = (grad_W1, grad_W2, grad_W3, grad_b1, grad_b2, grad_b3)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3f8512a-ec5b-49cc-adec-3f0d8bb5c729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_learning_without_vectorization(layers_dim, dataset, epoch, batch_size, learning_rate, vectorized=False):\n",
    "    param = init_W_b(layers_dim)\n",
    "    W1, W2, W3, b1, b2, b3 = param\n",
    "    total_cost_arr_in_batch = []\n",
    "    for _ in range(epoch):\n",
    "        # shuffle\n",
    "        np.random.shuffle(train_set)\n",
    "\n",
    "        for start in range(0, len(train_set), batch_size):\n",
    "            # make batch\n",
    "            stop = start + batch_size\n",
    "            batch = train_set[start:stop]\n",
    "\n",
    "            total_cost_in_batch = 0\n",
    "            grads = init_grad_W_b(layers_dim)\n",
    "\n",
    "            for image in batch:\n",
    "                label = image[1]\n",
    "                activations = feed_forward(image[0], param)\n",
    "                A3, *_ = activations\n",
    "                total_cost_in_batch += calculate_cost(A3, label)\n",
    "                \n",
    "                # backpropagation calculating\n",
    "                if not vectorized:\n",
    "                    grad_W3, grad_W2, grad_W1, grad_b3, grad_b2, grad_b1 = update_grad_W_b_in_layer(layers_dim, activations, param, image, grads)\n",
    "                else:\n",
    "                    grad_W3, grad_W2, grad_W1, grad_b3, grad_b2, grad_b1 = update_grad_W_b_in_layer_vectorized(layers_dim, activations, param, image, grads)\n",
    "                    \n",
    "            total_cost_arr_in_batch.append(total_cost_in_batch)\n",
    "            \n",
    "            # update Wi and bi (end of each batch)\n",
    "            W3, b3 = update_W_b(learning_rate, grad_W3, W3, grad_b3, b3, batch_size)\n",
    "            W2, b2 = update_W_b(learning_rate, grad_W2, W2, grad_b2, b2, batch_size)\n",
    "            W1, b1 = update_W_b(learning_rate, grad_W1, W1, grad_b1, b1, batch_size)\n",
    "\n",
    "    param = W1, W2, W3, b1, b2, b3\n",
    "    return param, total_cost_arr_in_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "369baddd-4399-4e2c-b334-835cc08e99eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 10\n",
    "learning_rate = 1\n",
    "epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bc8854-4277-4501-9afa-16313a50c3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "param, total_cost_arr_in_batch = start_learning_without_vectorization(layers_dim, train_set[0: 100], epoch, batch_size, learning_rate)  # this W,b after learning\n",
    "accuracy = calculate_accuracy(param, train_set[0: 100])\n",
    "print(\"Accuracy :\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a4a66a-9067-47ef-bc11-eb3e4598226b",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2d3453e-cd74-4a98-8ca9-c8aaa24460bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_grad_W_b_in_layer_vectorized(layers_dim, activations, param, image, grads):\n",
    "    n0, n1, n2, n3 = layers_dim\n",
    "    A3, Z3, A2, Z2, A1, Z1 = activations\n",
    "    W1, W2, W3, b1, b2, b3 = param\n",
    "    grad_W1, grad_W2, grad_W3, grad_b1, grad_b2, grad_b3 = grads\n",
    "    \n",
    "    grad_W3 += (2 * derivative_activation(Z3) * (A3 - image[1])) @ (np.transpose(A2))\n",
    "    grad_b3 += (2 * derivative_activation(Z3) * (A3 - image[1]))\n",
    "    grad_A2 = np.zeros((n_h2, 1))\n",
    "\n",
    "    grad_A2 = np.transpose(W3) @ (2 * derivative_activation(Z3, activationType) * (A3 - image[1]))\n",
    "\n",
    "    grad_W2 += (derivative_activation(Z2) * grad_A2) @ (np.transpose(A1))\n",
    "    grad_b2 += (derivative_activation(Z2) * grad_A2)\n",
    "    grad_A1 = np.zeros((n_h1, 1))\n",
    "    grad_A1 = np.transpose(W2) @ (derivative_activation(Z2, activationType) * grad_A2)\n",
    "\n",
    "    grad_W1 += (derivative_activation(Z1) * grad_A1) @ (np.transpose(image[0]))\n",
    "    grad_b1 += (derivative_activation(Z1) * grad_A1)\n",
    "\n",
    "    return grad_W3, grad_W2, grad_W1, grad_b3, grad_b2, grad_b1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a91a49e-8275-48fd-93a9-3e3d75c9094f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
